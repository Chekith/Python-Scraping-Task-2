{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9019c32-915b-4d04-809a-aefeacd530b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06d9963-0dfa-4a72-b972-e2ff7cdf6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d455efad-d246-4d8f-b4ee-9ffc9d552b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download CSV from Google Drive\n",
    "def download_csv_from_google_drive(url, output_path):\n",
    "    gdown.download(url, output_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2302435c-06e4-4da4-8f2b-d64ca749b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Read the CSV to get Twitter profile URLs\n",
    "def load_profile_urls(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df['twitter_profile_url'].tolist()  # Assuming the CSV has a column named 'twitter_profile_url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a16233d-fe70-4e59-8377-c6c726065031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Scrape Twitter profile details\n",
    "def scrape_twitter_profile(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Initializing profile data dictionary\n",
    "    profile_data = {\n",
    "        \"Bio\": None,\n",
    "        \"Following Count\": None,\n",
    "        \"Followers Count\": None,\n",
    "        \"Location\": None,\n",
    "        \"Website\": None\n",
    "    }\n",
    "     # Extracting the bio\n",
    "    bio_tag = soup.find('div', {'data-testid': 'UserDescription'})\n",
    "    profile_data['Bio'] = bio_tag.text if bio_tag else None\n",
    "     # Extracting the following count\n",
    "    following_tag = soup.find('a', {'href': f\"{url}/following\"})\n",
    "    profile_data['Following Count'] = following_tag.find('span').text if following_tag else None\n",
    "    # Extracting the followers count\n",
    "    followers_tag = soup.find('a', {'href': f\"{url}/followers\"})\n",
    "    profile_data['Followers Count'] = followers_tag.find('span').text if followers_tag else None\n",
    "     # Extracting location\n",
    "    location_tag = soup.find('span', {'data-testid': 'UserLocation'})\n",
    "    profile_data['Location'] = location_tag.text if location_tag else None\n",
    "    # Extracting website\n",
    "    website_tag = soup.find('a', {'data-testid': 'UserUrl'})\n",
    "    profile_data['Website'] = website_tag['href'] if website_tag else None\n",
    "    return profile_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42304c6a-6da3-4644-89ae-ce2603253366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save data to a CSV\n",
    "def save_data_to_csv(data, output_path):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "217ff3bc-caf5-4456-8009-b2725dcfe369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1PLYwrGn5YApyWU2QpjbdhM6tea0HuGq7\n",
      "To: C:\\Users\\user\\profiles.csv\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 492/492 [00:00<00:00, 449kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Download and load profile URLs\n",
    "    csv_url = 'https://drive.google.com/uc?id=1PLYwrGn5YApyWU2QpjbdhM6tea0HuGq7'  # Direct download link for Google Drive\n",
    "    input_csv = 'profiles.csv'\n",
    "    output_csv = 'twitter_profiles_data.csv'\n",
    "    # Download the CSV from Google Drive\n",
    "    download_csv_from_google_drive(csv_url, input_csv)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef31a6a4-fd1c-418f-bbf4-b4846a0c220e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the CSV file:                                       0\n",
      "0            https://twitter.com/GTNUK1\n",
      "1          https://twitter.com/whatsapp\n",
      "2     https://twitter.com/aacb_CBPTrade\n",
      "3        https://twitter.com/aacbdotcom\n",
      "4  https://twitter.com/@AAWindowPRODUCT\n",
      "Loaded profile URLs: ['https://twitter.com/GTNUK1', 'https://twitter.com/whatsapp', 'https://twitter.com/aacb_CBPTrade', 'https://twitter.com/aacbdotcom', 'https://twitter.com/@AAWindowPRODUCT', 'https://www.twitter.com/aandb_kia', 'https://twitter.com/ABHomeInc', 'https://twitter.com/Abrepro', 'http://www.twitter.com', 'https://twitter.com/ACChristofiLtd', 'https://twitter.com/aeclothing1', 'http://www.twitter.com/', 'https://twitter.com/AETechnologies1', 'http://www.twitter.com/wix', 'https://twitter.com/AGInsuranceLLC']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV without headers to get Twitter profile URLs\n",
    "def load_profile_urls(csv_path):\n",
    "    try:\n",
    "        # Read the CSV file without headers\n",
    "        df = pd.read_csv(csv_path, header=None)  # header=None treats the first row as data, not as column names\n",
    "        \n",
    "        # Print the first few rows to verify the data\n",
    "        print(\"First few rows of the CSV file:\", df.head())\n",
    "        \n",
    "        # Access the first column (index 0) as it contains the URLs\n",
    "        profile_urls = df[0].tolist()  # Assuming URLs are in the first column\n",
    "        \n",
    "        return profile_urls\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_path}' was not found.\")\n",
    "        return []\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: The file is empty.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Usage\n",
    "input_csv = 'profiles.csv'  # Path to your local CSV file\n",
    "profile_urls = load_profile_urls(input_csv)\n",
    "print(\"Loaded profile URLs:\", profile_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "006b6852-6f37-4e69-90d1-5782d364dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://twitter.com/GTNUK1\n",
      "Scraping https://twitter.com/whatsapp\n",
      "Scraping https://twitter.com/aacb_CBPTrade\n",
      "Scraping https://twitter.com/aacbdotcom\n",
      "Scraping https://twitter.com/@AAWindowPRODUCT\n",
      "Scraping https://www.twitter.com/aandb_kia\n",
      "Scraping https://twitter.com/ABHomeInc\n",
      "Scraping https://twitter.com/Abrepro\n",
      "Scraping http://www.twitter.com\n",
      "Scraping https://twitter.com/ACChristofiLtd\n",
      "Scraping https://twitter.com/aeclothing1\n",
      "Scraping http://www.twitter.com/\n",
      "Scraping https://twitter.com/AETechnologies1\n",
      "Scraping http://www.twitter.com/wix\n",
      "Scraping https://twitter.com/AGInsuranceLLC\n"
     ]
    }
   ],
   "source": [
    " # Scrape each profile and collect data\n",
    "scraped_data = []\n",
    "for url in profile_urls:\n",
    "    print(f\"Scraping {url}\")\n",
    "    data = scrape_twitter_profile(url)\n",
    "    data['URL'] = url\n",
    "    scraped_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc916366-07f3-4f19-abf7-dd8cec4eec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to twitter_profiles_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save all collected data to a CSV file\n",
    "save_data_to_csv(scraped_data, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72cb36-5de2-48e0-be81-bf5c7e779a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
